{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "The following workflow will be executed in this notebook:\n",
    "1. Import common libraries for machine learning pipelines\n",
    "2. Load the dataset\n",
    "3. Create a Customer Text Transformer\n",
    "4. Build a machine learning pipeline\n",
    "5. Test model performance - 1st iteration\n",
    "6. Hyperparameter tuning: Improve Model Performance\n",
    "7. Test model performance - 2nd iteration\n",
    "8. Train on best hyperparameters\n",
    "9. Compare Training Results \n",
    "10. Improve Model Performance: Train another classifer\n",
    "11. Final Comparison of all Models - 3rd iteration\n",
    "12. Export the model as a pickle file\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import re # for regular expressions\n",
    "import numpy as np # numeric python, vector operations\n",
    "import pandas as pd # data manipulation\n",
    "from nltk.corpus import stopwords # natural language tool kit: stopwords\n",
    "from nltk.tokenize import word_tokenize # natural language tool kit: word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer # natural language tool kit: lemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer # for text processing\n",
    "from sklearn.model_selection import train_test_split # for splitting data into training and testing\n",
    "from sklearn.multioutput import MultiOutputClassifier   # for multi-output classification\n",
    "from sklearn.ensemble import RandomForestClassifier # for random forest classifier\n",
    "from sklearn.pipeline import Pipeline   # for creating a pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score # for model evaluation\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2. Load the dataset\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///../data/disaster_response.db')\n",
    "df = pd.read_sql_table('features', engine) \n",
    "X = df['message']\n",
    "y = df.iloc[:,4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create a Customer Text Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create custom text transformer\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    This function takes a text and returns a list of cleaned and tokenized words.\n",
    "    Designed to be used in a pipeline, with the CountVectorizer and TfidfTransformer objects\n",
    "\n",
    "    Args:\n",
    "    text: str: a string of text to be tokenized\n",
    "\n",
    "    Returns:\n",
    "    lemmed: list: a list of cleaned and tokenized words\n",
    "    \"\"\"\n",
    "    # Normalize text\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    \n",
    "    # Tokenize text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "    \n",
    "    # Lemmatization\n",
    "    words = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
    "\n",
    "    # Stemming (not used, does not improve performance)\n",
    "    # stemmed = [PorterStemmer().stem(w) for w in lemmed]\n",
    "    \n",
    "    return words\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Tokenization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In every alley, every corner and every lane of Mogadishu, a mujahid (Islamist fighter) lies in wait for them, the rebels added.\n",
      "['every', 'alley', 'every', 'corner', 'every', 'lane', 'mogadishu', 'mujahid', 'islamist', 'fighter', 'lie', 'wait', 'rebel', 'added'] \n",
      "\n",
      "How is the capital now, because I learned that there were people enjoying themselves pillaging the stores and businesses that were still standing \n",
      "['capital', 'learned', 'people', 'enjoying', 'pillaging', 'store', 'business', 'still', 'standing'] \n",
      "\n",
      "ANTANANARIVO, March 11 (AFP) - Thirty-six people were killed and 42 were missing in north Madagascar after a storm lashed the region at the weekend, while scores more were feared drowned at sea, rescue services said Thursday.\n",
      "['antananarivo', 'march', '11', 'afp', 'thirty', 'six', 'people', 'killed', '42', 'missing', 'north', 'madagascar', 'storm', 'lashed', 'region', 'weekend', 'score', 'feared', 'drowned', 'sea', 'rescue', 'service', 'said', 'thursday'] \n",
      "\n",
      "As a result of the disruption in safe drinking water supplies in drought-affected areas, the population is now exposed to higher risks of water-borne diseases.\n",
      "['result', 'disruption', 'safe', 'drinking', 'water', 'supply', 'drought', 'affected', 'area', 'population', 'exposed', 'higher', 'risk', 'water', 'borne', 'disease'] \n",
      "\n",
      "A 500-meter section of the railroad was submerged beneath 20 centimeters of floodwater following torrential rain throughout Monday.\n",
      "['500', 'meter', 'section', 'railroad', 'submerged', 'beneath', '20', 'centimeter', 'floodwater', 'following', 'torrential', 'rain', 'throughout', 'monday'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test out function\n",
    "for message in X.sample(5):\n",
    "    tokens = tokenize(message)\n",
    "    print(message)\n",
    "    print(tokens, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline will have 3 steps\n",
    "# 1. CountVectorizer - Convert a collection of text documents to a matrix of token counts\n",
    "# 2. TfidfTransformer - Transform a count matrix to a normalized tf or tf-idf representation\n",
    "# 3. MultiOutputClassifier - This is a simple meta-estimator for fitting one classifier per target.\n",
    "pipeline = Pipeline([ \n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)), # here is where you use the custom text transformer\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(verbose=1)))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train pipeline with the Benchmark model (we will look to improve through more iterations)\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(tokenizer=<function tokenize at 0x1651182c0>)),\n",
       "  ('tfidf', TfidfTransformer()),\n",
       "  ('clf', MultiOutputClassifier(estimator=RandomForestClassifier(verbose=1)))],\n",
       " 'verbose': False,\n",
       " 'vect': CountVectorizer(tokenizer=<function tokenize at 0x1651182c0>),\n",
       " 'tfidf': TfidfTransformer(),\n",
       " 'clf': MultiOutputClassifier(estimator=RandomForestClassifier(verbose=1)),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__estimator__bootstrap': True,\n",
       " 'clf__estimator__ccp_alpha': 0.0,\n",
       " 'clf__estimator__class_weight': None,\n",
       " 'clf__estimator__criterion': 'gini',\n",
       " 'clf__estimator__max_depth': None,\n",
       " 'clf__estimator__max_features': 'sqrt',\n",
       " 'clf__estimator__max_leaf_nodes': None,\n",
       " 'clf__estimator__max_samples': None,\n",
       " 'clf__estimator__min_impurity_decrease': 0.0,\n",
       " 'clf__estimator__min_samples_leaf': 1,\n",
       " 'clf__estimator__min_samples_split': 2,\n",
       " 'clf__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__estimator__monotonic_cst': None,\n",
       " 'clf__estimator__n_estimators': 100,\n",
       " 'clf__estimator__n_jobs': None,\n",
       " 'clf__estimator__oob_score': False,\n",
       " 'clf__estimator__random_state': None,\n",
       " 'clf__estimator__verbose': 1,\n",
       " 'clf__estimator__warm_start': False,\n",
       " 'clf__estimator': RandomForestClassifier(verbose=1),\n",
       " 'clf__n_jobs': None}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show params for benchmark model\n",
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training observations: 20972\n",
      "total testing observations: 5244\n"
     ]
    }
   ],
   "source": [
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# show shape of the different datsets\n",
    "print(f'total training observations: {X_train.shape[0]}')\n",
    "print(f'total testing observations: {X_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:   11.4s\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    9.3s\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:   11.8s\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    3.2s\n"
     ]
    }
   ],
   "source": [
    "# train classifier\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model - 1st iteration\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create reusable function to evaluate model performance for all 36 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_precision(df,iteration):\n",
    "    \"\"\"\n",
    "    Extract the precision from the results dataframe and annotates columns for comparison\n",
    "\n",
    "    Args:\n",
    "    df: DataFrame: the results dataframe\n",
    "    iteration: int: the iteration number\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: a dataframe containing the precision and support metrics\n",
    "    \"\"\"\n",
    "    # filter the results to only include the precision and support metrics\n",
    "    df = df.loc[df.metric=='1',['label','precision']].reset_index(drop=True)\n",
    "\n",
    "    # add _iteration to the column names, starting with column 1\n",
    "    df.columns = ['label', f'precision_{iteration}']\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def evaluate_models(X_test,y_test,estimator,iteration):\n",
    "    \"\"\"\n",
    "        Evaluate the model by calculating the precision, recall, and f1-score for each label\n",
    "    \n",
    "        Args:\n",
    "        X_test: the test features\n",
    "        y_test: the test labels\n",
    "        estimator: the trained model\n",
    "    \n",
    "        Returns:\n",
    "        results: a dataframe containing the precision, recall, and f1-score for each label\n",
    "    \"\"\"\n",
    "    # import report that captures appropriate metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "    # predict on test data\n",
    "    y_pred = estimator.predict(X_test)\n",
    "\n",
    "    # initialize an empty dataframe to store the results\n",
    "    results = pd.DataFrame()\n",
    "\n",
    "    # calculate and store the precision, recall, and f1-score for each label\n",
    "    for i, col in enumerate(y_test.columns):\n",
    "        report = classification_report(y_test[col], y_pred[:, i], output_dict=True)\n",
    "        df = pd.DataFrame(report).transpose()\n",
    "        df['label'] = col\n",
    "        results = pd.concat([results, df])\n",
    "\n",
    "    # reset the index of the results dataframe\n",
    "    results.reset_index(inplace=True)\n",
    "    results.rename(columns={'index': 'metric'}, inplace=True)\n",
    "\n",
    "    # rearrange the columns to put 'label' first\n",
    "    cols = ['label'] + [col for col in results.columns if col != 'label']\n",
    "    results = results[cols]\n",
    "\n",
    "    # round the results to 2 decimal places\n",
    "    results = results.round(2)\n",
    "\n",
    "    # convert the 'support' column to integers\n",
    "    results.support = results.support.astype(int)\n",
    "\n",
    "    # extract the precision from the results dataframe\n",
    "    results = extract_precision(results,iteration)\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model, 1st iteration\n",
    "results1 = evaluate_models(X_test,y_test,pipeline,1)\n",
    "\n",
    "results1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use grid search to find better parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# we are limiting the grid to these options, which will take 2 hrs to train\n",
    "# adding parameters will increase time exponentially\n",
    "parameters = {\n",
    "    'clf__estimator__n_estimators':[5, 10],\n",
    "    'clf__estimator__max_depth': [3, 5],\n",
    "    'clf__estimator__min_samples_split': [2, 4]\n",
    "}\n",
    "\n",
    "# instantiate grid search object with appropriate parameters\n",
    "cv = GridSearchCV(pipeline, \n",
    "                  param_grid=parameters, \n",
    "                  verbose=1, \n",
    "                  cv=5, \n",
    "                  n_jobs=1, \n",
    "                  return_train_score=True, \n",
    "                  scoring='f1_weighted')\n",
    "\n",
    "# train the model\n",
    "cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model - 2nd iteration\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model - 2nd iteration    \n",
    "results2 = evaluate_models(X_test,y_test,cv,2)\n",
    "\n",
    "results2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the best parameters\n",
    "print('best parameters:', cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Train the model on the best parameters and evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pipeline\n",
    "pipeline = Pipeline([ \n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(verbose=1)))\n",
    "])\n",
    "\n",
    "# set pareters to best parameters from grid search\n",
    "pipeline.set_params(**cv.best_params_)\n",
    "\n",
    "# fit the model\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model - 3rd iteration\n",
    "results3 = evaluate_models(X_test,y_test,pipeline,3)\n",
    "\n",
    "results3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Compare training results with the benchmark model\n",
    "The second model is clearly not catching the true positive classes<br>\n",
    "We will attempt to train on a different classifer to see if we can improve accuracy<br>\n",
    "<br>\n",
    "Now we will look closer at the different model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consolidate results between 3 models\n",
    "from functools import reduce\n",
    "\n",
    "# combine the precision dataframes\n",
    "dfs = [results1, results2, results3]\n",
    "\n",
    "# merge the dataframes on the 'label' column\n",
    "results = reduce(lambda left, right: pd.merge(left, right, on='label'), dfs)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.  Improve Model Performance: Train another classifer\n",
    "We will attempt to train the same pipline with an XGboost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the XGBoost classifier for multiclass objective function\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(XGBClassifier()))\n",
    "\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "results4 = evaluate_models(X_test, y_test, pipeline,4)\n",
    "\n",
    "results4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.  Final Comparison of all Models - 3rd iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the results with other iterations\n",
    "results = pd.merge(results, results4, on='label', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# extract the average precision per model \n",
    "avg_precision = (\n",
    "                        results.iloc[:,1:]\n",
    "                        .agg({col: 'mean' for col in results.columns[1:]})\n",
    "                        .to_frame()\n",
    "                        .rename(columns={0: 'avg_precision'})\n",
    "                        .rename_axis('metric')\n",
    "                        .reset_index()\n",
    "                )\n",
    "\n",
    "# drop the last row, as it is not needed\n",
    "avg_precision = avg_precision.drop(avg_precision.index[4])\n",
    "\n",
    "avg_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as mpatches  # import the patches module\n",
    "\n",
    "# create a new column for colors\n",
    "avg_precision['color'] = avg_precision['model']\n",
    "\n",
    "# create a custom color palette\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "barplot = sns.barplot(x='model', y='avg_precision', hue='color', data=avg_precision, ax=ax, dodge=False, palette='tab10')\n",
    "\n",
    "# add labels and title\n",
    "plt.title('Average Precision by Model')\n",
    "plt.ylabel('Average Precision')\n",
    "\n",
    "# change the x labels to match the legend and orient them at a 45-degree angle\n",
    "x_labels = ['Random Forest 1', 'Random Forest 2', 'Random Forest 3', 'XGBoost']\n",
    "ax.set_xticklabels(x_labels, rotation=45, ha='right')  # adjust the horizontal alignment\n",
    "\n",
    "# create a custom legend\n",
    "legend_labels = ['Random Forest 1', 'Random Forest 2', 'Random Forest 3', 'XGBoost']\n",
    "legend_colors = [barplot.patches[i].get_facecolor() for i in range(len(legend_labels))]\n",
    "legend = plt.legend(handles=[mpatches.Patch(color=c) for c in legend_colors], labels=legend_labels, bbox_to_anchor=(1.05, 1), loc='upper left')  # use mpatches.Patch and move the legend outside of the plot\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the average precision for each model\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# set the rc parameters for font weight, size, and spacing\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['lines.linewidth'] = 2\n",
    "\n",
    "# set the colormap\n",
    "cmap = cm.get_cmap('tab10')  # get the 'tab10' colormap\n",
    "\n",
    "# set the figure size\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "# create stacked horizontal bar plots with a different color palette\n",
    "plt.barh(results.label, results.precision_1, color=cmap(0), alpha=0.5, label='Random Forest 1')\n",
    "plt.barh(results.label, results.precision_2, left=results.precision_1, color=cmap(1), alpha=0.5, label='Random Forest 2')\n",
    "plt.barh(results.label, results.precision_3, left=results.precision_1 + results.precision_2, color=cmap(2), alpha=0.5, label='Random Forest 3')\n",
    "plt.barh(results.label, results.precision_4, left=results.precision_1 + results.precision_2 + results.precision_3, color=cmap(3), alpha=0.5, label='XGBoost')\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Model Training Iterations\n",
    "1.  Benchmark model: RandomForestClassifier - showed 95% accuracy\n",
    "2.  Grid Search: We tried 4 different sets of hyperparameters with cross validation.  However, the model seriously underfit and could not detect most of the postive classes.\n",
    "3.  XGBoost - We tried XGBoost with GridSearch and it showed 95% accuracy.  We saved this model as the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export your model as a pickel file\n",
    "import joblib\n",
    "with open('classifier.pkl', 'wb') as file:\n",
    "    joblib.dump(pipeline, file, compress=5)\n",
    "\n",
    "# load model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train_classifier.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
